spring:
  application:
    name: mod-linked-data-import
  batch:
    jdbc:
      initialize-schema: never
  cloud:
    openfeign:
      okhttp:
        enabled: true
  datasource:
    username: ${DB_USERNAME:postgres}
    password: ${DB_PASSWORD:postgres}
    url: jdbc:postgresql://${DB_HOST:postgres}:${DB_PORT:5432}/${DB_DATABASE:okapi_modules}
  jpa:
    hibernate:
      ddl-auto: none
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
        event:
          merge:
            entity_copy_observer: allow
  kafka:
    bootstrap-servers: ${KAFKA_HOST:kafka}:${KAFKA_PORT:9092}
    consumer:
      max-poll-records: ${KAFKA_CONSUMER_MAX_POLL_RECORDS:200}
    security:
      protocol: ${KAFKA_SECURITY_PROTOCOL:PLAINTEXT}
    ssl:
      key-store-password: ${KAFKA_SSL_KEYSTORE_PASSWORD:}
      key-store-location: ${KAFKA_SSL_KEYSTORE_LOCATION:}
      trust-store-password: ${KAFKA_SSL_TRUSTSTORE_PASSWORD:}
      trust-store-location: ${KAFKA_SSL_TRUSTSTORE_LOCATION:}
    producer:
      acks: all
      properties:
        enable.idempotence: true
        max.in.flight.requests.per.connection: 5
        retries: 5
        spring.json.add.type.headers: false
  liquibase:
    change-log: classpath:changelog/changelog-master.xml
  main:
    allow-bean-definition-overriding: true

folio:
  environment: ${ENV:folio}
  okapiUrl: ${okapi.url}
  tenant:
    validation:
      enabled: true
  logging:
    request:
      enabled: true
    feign:
      enabled: true
  retry:
    enabled: true
  kafka:
    retry-interval-ms: ${KAFKA_RETRY_INTERVAL_MS:2000}
    retry-delivery-attempts: ${KAFKA_RETRY_DELIVERY_ATTEMPTS:6}
    listener:
      import-result-event:
        concurrency: ${KAFKA_IMPORT_RESULT_EVENT_CONCURRENCY:10}
        topic-pattern: ${KAFKA_IMPORT_RESULT_EVENT_TOPIC_PATTERN:(${folio.environment}\.)(.*\.)${mod-linked-data-import.kafka.topic.linked_data_import-result}}
        group-id: ${folio.environment}-linked-data-import-result-event-group
    topics:
      - name: ${mod-linked-data-import.kafka.topic.linked_data_import-output}
        numPartitions: ${KAFKA_LINKED_DATA_IMPORT_OUTPUT_TOPIC_PARTITIONS:3}
        replicationFactor: ${KAFKA_LINKED_DATA_IMPORT_OUTPUT_TOPIC_REPLICATION_FACTOR:}
      - name: ${mod-linked-data-import.kafka.topic.linked_data_import-result}
        numPartitions: ${KAFKA_LINKED_DATA_IMPORT_RESULT_TOPIC_PARTITIONS:3}
        replicationFactor: ${KAFKA_LINKED_DATA_IMPORT_RESULT_TOPIC_REPLICATION_FACTOR:}
  remote-storage:
    endpoint: ${S3_URL:http://localhost:9000/}
    region: ${S3_REGION:}
    bucket: ${S3_BUCKET:}
    accessKey: ${S3_ACCESS_KEY_ID:}
    secretKey: ${S3_SECRET_ACCESS_KEY:}
    awsSdk: ${S3_IS_AWS:false}

mod-linked-data-import:
  job-pool-size: ${JOB_POOL_SIZE:1}
  chunk-size: ${CHUNK_SIZE:1000}
  output-chunk-size: ${OUTPUT_CHUNK_SIZE:100}
  wait-for-processing-lines-per-minute: ${WAIT_FOR_PROCESSING_LINES_PER_MINUTE:500}
  process-file:
    max-pool-size: ${PROCESS_FILE_MAX_POOL_SIZE:1000}
  kafka:
    topic:
      linked_data_import-output: ${KAFKA_LINKED_DATA_IMPORT_OUTPUT_TOPIC:linked_data_import.output}
      linked_data_import-result: ${KAFKA_LINKED_DATA_IMPORT_RESULT_TOPIC:linked_data_import.result}

management:
  endpoints:
    web:
      exposure:
        include: info,health,liquibase,threaddump,heapdump,loggers,env,httptrace,metrics,prometheus
      base-path: /admin
  endpoint:
    loggers:
      access: unrestricted
  influx:
    metrics:
      export:
        enabled: false

server:
  port: ${SERVER_PORT:8081}
